{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ba21530-f55f-46f3-9654-1a69d5e84360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def dwise_conv(ch_in, stride=1):\n",
    "    return (\n",
    "        nn.Sequential(\n",
    "            #depthwise\n",
    "            nn.Conv2d(ch_in, ch_in, kernel_size=3, padding=1, stride=stride, groups=ch_in, bias=False),\n",
    "            nn.BatchNorm2d(ch_in),\n",
    "            nn.ReLU6(inplace=True),\n",
    "        )\n",
    "    )\n",
    "\n",
    "def conv1x1(ch_in, ch_out):\n",
    "    return (\n",
    "        nn.Sequential(\n",
    "            nn.Conv2d(ch_in, ch_out, kernel_size=1, padding=0, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "    )\n",
    "\n",
    "def conv3x3(ch_in, ch_out, stride):\n",
    "    return (\n",
    "        nn.Sequential(\n",
    "            nn.Conv2d(ch_in, ch_out, kernel_size=3, padding=1, stride=stride, bias=False),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "    )\n",
    "\n",
    "class InvertedBlock(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out, expand_ratio, stride):\n",
    "        super(InvertedBlock, self).__init__()\n",
    "\n",
    "        self.stride = stride\n",
    "        assert stride in [1,2]\n",
    "\n",
    "        hidden_dim = ch_in * expand_ratio\n",
    "\n",
    "        self.use_res_connect = self.stride==1 and ch_in==ch_out\n",
    "\n",
    "        layers = []\n",
    "        if expand_ratio != 1:\n",
    "            layers.append(conv1x1(ch_in, hidden_dim))\n",
    "        layers.extend([\n",
    "            #dw\n",
    "            dwise_conv(hidden_dim, stride=stride),\n",
    "            #pw\n",
    "            conv1x1(hidden_dim, ch_out)\n",
    "        ])\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return x + self.layers(x)\n",
    "        else:\n",
    "            return self.layers(x)\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, ch_in=3, n_classes=1000):\n",
    "        super(MobileNetV2, self).__init__()\n",
    "\n",
    "        self.configs=[\n",
    "            # t, c, n, s\n",
    "            [1, 16, 1, 1],\n",
    "            [6, 24, 2, 2],\n",
    "            [6, 32, 3, 2],\n",
    "            [6, 64, 4, 2],\n",
    "            [6, 96, 3, 1],\n",
    "            [6, 160, 3, 2],\n",
    "            [6, 320, 1, 1]\n",
    "        ]\n",
    "\n",
    "        self.stem_conv = conv3x3(ch_in, 32, stride=2)\n",
    "\n",
    "        layers = []\n",
    "        input_channel = 32\n",
    "        for t, c, n, s in self.configs:\n",
    "            for i in range(n):\n",
    "                stride = s if i == 0 else 1\n",
    "                layers.append(InvertedBlock(ch_in=input_channel, ch_out=c, expand_ratio=t, stride=stride))\n",
    "                input_channel = c\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "        self.last_conv = conv1x1(input_channel, 1280)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout2d(0.2),\n",
    "            nn.Linear(1280, 1)\n",
    "        )\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem_conv(x)\n",
    "        x = self.layers(x)\n",
    "        x = self.last_conv(x)\n",
    "        x = self.avg_pool(x).view(-1, 1280)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45152e2e-84a5-4bc3-b499-da9a10d7ff2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import copy\n",
    "import lightning as L\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import datasets\n",
    "import albumentations as A\n",
    "import shutil\n",
    "import cv2\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "from urllib.request import urlretrieve\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from torchvision import models\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import optim, nn, Tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchmetrics.image import StructuralSimilarityIndexMeasure\n",
    "from pytorch_msssim import ssim\n",
    "from torchvision.transforms.functional import resize\n",
    "\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "def images_to_probs(net, images):\n",
    "    '''\n",
    "    Generates predictions and corresponding probabilities from a trained\n",
    "    network and a list of images\n",
    "    '''\n",
    "    output = net(images)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, preds_tensor = torch.max(output, 1)\n",
    "    preds = np.squeeze(preds_tensor.cpu().numpy())\n",
    "    return preds, [F.softmax(el, dim=0)[i].item() for i, el in zip(preds, output)]\n",
    "\n",
    "\n",
    "def plot_classes_preds(net, images, labels):\n",
    "    '''\n",
    "    Generates matplotlib Figure using a trained network, along with images\n",
    "    and labels from a batch, that shows the network's top prediction along\n",
    "    with its probability, alongside the actual label, coloring this\n",
    "    information based on whether the prediction was correct or not.\n",
    "    Uses the \"images_to_probs\" function.\n",
    "    '''\n",
    "    preds, probs = images_to_probs(net, images)\n",
    "    # plot the images in the batch, along with predicted and true labels\n",
    "    fig = plt.figure(figsize=(12, 48))\n",
    "    for idx in np.arange(len(images)):\n",
    "        ax = fig.add_subplot(1, 4, idx+1, xticks=[], yticks=[])\n",
    "        matplotlib_imshow(images[idx].cpu(), one_channel=True)\n",
    "        ax.set_title(\"{0}, {1:.1f}%\\n(label: {2})\".format(\n",
    "            str(preds[idx]),\n",
    "            probs[idx] * 100.0,\n",
    "            str(labels[idx])),\n",
    "                    color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))\n",
    "    return fig\n",
    "\n",
    "\n",
    "class MobileNetTrainer(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = MobileNetV2(ch_in=3, n_classes=2)\n",
    "        self.loss=nn.BCEWithLogitsLoss()\n",
    "        self.ssim = StructuralSimilarityIndexMeasure(data_range=1.0)\n",
    "        torch.set_float32_matmul_precision('high')\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        return self.model(inputs)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs = batch[0]\n",
    "        target = batch[1]\n",
    "        output = self(inputs.to(dtype=torch.float32)).squeeze()\n",
    "        loss = self.loss(output, target)\n",
    "        self.log_dict({\"train_loss\": loss}, prog_bar=True)\n",
    "        return loss\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs = batch[0]\n",
    "        target = batch[1]\n",
    "        output = self(inputs.to(dtype=torch.float32)).squeeze()\n",
    "        loss = self.loss(output, target)\n",
    "        self.log_dict({\"val_loss\": loss}, prog_bar=True)\n",
    "        return loss\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.model.parameters(), lr=1e-5)\n",
    "\n",
    "class LogEpochValidationImage(L.Callback):\n",
    "    def __init__(self,reference):\n",
    "        super().__init__()\n",
    "        self.reference=reference\n",
    "        \n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        tensorboard = trainer.logger.experiment\n",
    "        z = pl_module.model(self.reference['image'][0:10].to('cuda').to(dtype=torch.float32))\n",
    "        tensorboard.add_figure('predictions vs. actuals',\n",
    "                            plot_classes_preds(pl_module.model, self.reference['image'][0:10].to('cuda').to(dtype=torch.float32),  self.reference['label'][0:10]),\n",
    "                            global_step=trainer.current_epoch )\n",
    "\n",
    "        # tensorboard.add_image(\"val\",z.squeeze(0).squeeze(0),trainer.current_epoch,dataformats=\"HW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16bd26a-274a-468a-870d-eddfb1b09edc",
   "metadata": {},
   "source": [
    "# Data pre-processing\n",
    "\n",
    "For the needs of the project the [Cats vs Dogs](https://www.kaggle.com/datasets/shaunthesheep/microsoft-catsvsdogs-dataset) was chosen due to its relatively small size compared to competitors (e.g. ImageNet with 1.2 million of images).\n",
    "Some files in the dataset are broken, so we will use only those image files that OpenCV could load correctly.\n",
    "\n",
    "## Dataset\n",
    "We will use `20000` images for training, `4936` images for validation, and `10` images for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5ca9c8b-dd81-469a-9919-58e08f109995",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TqdmUpTo(tqdm):\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)\n",
    "\n",
    "\n",
    "def download_url(url, filepath):\n",
    "    directory = os.path.dirname(os.path.abspath(filepath))\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    if os.path.exists(filepath):\n",
    "        print(\"Filepath already exists. Skipping download.\")\n",
    "        return\n",
    "\n",
    "    with TqdmUpTo(unit=\"B\", unit_scale=True, unit_divisor=1024, miniters=1, desc=os.path.basename(filepath)) as t:\n",
    "        urlretrieve(url, filename=filepath, reporthook=t.update_to, data=None)\n",
    "        t.total = t.n\n",
    "\n",
    "\n",
    "def extract_archive(filepath):\n",
    "    extract_dir = os.path.dirname(os.path.abspath(filepath))\n",
    "    shutil.unpack_archive(filepath, extract_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98d67c36-ab8d-45d8-b574-4c437f2eb121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filepath already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "dataset_directory = \"datasets/cats-vs-dogs\"\n",
    "filepath = os.path.join(dataset_directory, \"kagglecatsanddogs_3367a.zip\")\n",
    "download_url(\n",
    "    url=\"https://storage.googleapis.com/kaggle-data-sets/550917/1003830/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20240528%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240528T221554Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=34e1d50ea8778017947c808fd20beb5d34d8e0ba95fe1a6ad46ec6ac256af76ab05826955e4932f5b389be120669ce767137c29078a42a5b6fe8c753be165ccb904c7ee7db0162bc4ae8445637541981746a808ce387d9de24aa8504bbbbc39d60181d6e0413fa6a3b8c8be8575eba0a99fceff48eb64dd775e72da3c9f9b3e8402d2df143ec5b87b016181e82f71989b434c61bc15a2023879c70c005d8a90790e2582cd706b2a88eb787c1f2ee65335f6470e8f74e985b0c5a198b7a0c2a7821edc01da5f82a3f978563fbcef7d02182e4a732b014e14279e7473032a51e4c5cf2a3653660bac3ed45cff93f44aea5bd9f129cfa2e5275930ce8c2b2c2e941\",\n",
    "    filepath=filepath,\n",
    ")\n",
    "extract_archive(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86928ff2-90ad-445f-858b-95cfff18d7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 239 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: 214 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: 128 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: 99 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: 1153 extraneous bytes before marker 0xd9\n"
     ]
    }
   ],
   "source": [
    "root_directory = os.path.join(dataset_directory, \"PetImages\")\n",
    "\n",
    "cat_directory = os.path.join(root_directory, \"Cat\")\n",
    "dog_directory = os.path.join(root_directory, \"Dog\")\n",
    "\n",
    "cat_images_filepaths = sorted([os.path.join(cat_directory, f) for f in os.listdir(cat_directory)])\n",
    "dog_images_filepaths = sorted([os.path.join(dog_directory, f) for f in os.listdir(dog_directory)])\n",
    "images_filepaths = [*cat_images_filepaths, *dog_images_filepaths]\n",
    "correct_images_filepaths = [i for i in images_filepaths if cv2.imread(i) is not None]\n",
    "\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(correct_images_filepaths)\n",
    "train_images_filepaths = correct_images_filepaths[:20000]\n",
    "val_images_filepaths = correct_images_filepaths[20000:-10]\n",
    "test_images_filepaths = correct_images_filepaths[-10:]\n",
    "print(len(train_images_filepaths), len(val_images_filepaths), len(test_images_filepaths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d8fd6c-83c1-46ee-8c54-841fcedb19f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image_grid(images_filepaths, predicted_labels=(), cols=5):\n",
    "    rows = len(images_filepaths) // cols\n",
    "    figure, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(12, 6))\n",
    "    for i, image_filepath in enumerate(images_filepaths):\n",
    "        image = cv2.imread(image_filepath)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        true_label = os.path.normpath(image_filepath).split(os.sep)[-2]\n",
    "        predicted_label = predicted_labels[i] if predicted_labels else true_label\n",
    "        color = \"green\" if true_label == predicted_label else \"red\"\n",
    "        ax.ravel()[i].imshow(image)\n",
    "        ax.ravel()[i].set_title(predicted_label, color=color)\n",
    "        ax.ravel()[i].set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "display_image_grid(test_images_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a0d5b3-1ca6-46e4-ae60-0868e7e7309d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatsVsDogsDataset(Dataset):\n",
    "    def __init__(self, images_filepaths, transform=None, corruption_rate=None):\n",
    "        self.images_filepaths = images_filepaths\n",
    "        self.transform = transform\n",
    "        self.corruption_rate = 0.0 if corruption_rate is None else corruption_rate\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_filepaths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_filepath = self.images_filepaths[idx]\n",
    "        image = cv2.imread(image_filepath)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if os.path.normpath(image_filepath).split(os.sep)[-2] == \"Cat\":\n",
    "            label = 1.0\n",
    "        else:\n",
    "            label = 0.0\n",
    "        if np.random.random() < self.corruption_rate:\n",
    "            label = (label + 1.0) % 2\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image=image)[\"image\"]\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b80593-c7ab-4351-ace0-5176ee7d7477",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa442841-106c-4f8b-93cb-48007f2ddae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_transform = A.Compose(\n",
    "    [\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset_noise_transform = A.Compose(\n",
    "    [\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.PixelDropout(dropout_prob=0.1),\n",
    "                A.MultiplicativeNoise(multiplier=(0.4, 1.2), elementwise=True),\n",
    "                A.GaussNoise(var_limit=(350, 500)),\n",
    "                A.ISONoise(color_shift=(0.09, 0.5), intensity=(0.1, 0.95)),\n",
    "            ],\n",
    "            p=0.7,\n",
    "        ),\n",
    "        A.Downscale(scale_min=0.45, scale_max=0.85, p=0.5),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset_augmented_transform = A.Compose(\n",
    "    [\n",
    "        A.SmallestMaxSize(max_size=160),\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "        A.RandomCrop(height=128, width=128),\n",
    "        A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset = CatsVsDogsDataset(images_filepaths=train_images_filepaths, transform=train_dataset_transform)\n",
    "train_wrong_labels_dataset = CatsVsDogsDataset(images_filepaths=train_images_filepaths, transform=train_dataset_transform, corruption_rate=0.2)\n",
    "train_noise_dataset = CatsVsDogsDataset(images_filepaths=train_images_filepaths, transform=train_dataset_noise_transform)\n",
    "train_augmented_dataset = CatsVsDogsDataset(images_filepaths=train_images_filepaths, transform=train_dataset_augmented_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253ab15e-faae-4d0f-bb2f-23c57d42fba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transform = A.Compose(\n",
    "    [\n",
    "        A.SmallestMaxSize(max_size=160),\n",
    "        A.CenterCrop(height=128, width=128),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "val_dataset = CatsVsDogsDataset(images_filepaths=val_images_filepaths, transform=val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5974d553-8bc0-40d9-ad31-b73490fe78a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_augmentations(dataset, title, idx=0, samples=10, cols=5):\n",
    "    dataset = copy.deepcopy(dataset)\n",
    "    transforms = [t for t in dataset.transform if not isinstance(t, (A.Normalize, ToTensorV2))]\n",
    "    if transforms:\n",
    "        dataset.transform = A.Compose(transforms)\n",
    "    else:\n",
    "        dataset.transform = None\n",
    "    rows = samples // cols\n",
    "    figure, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(12, 6))\n",
    "    figure.suptitle(title, fontsize=14)\n",
    "    for i in range(samples):\n",
    "        image, label = dataset[idx]\n",
    "        ax.ravel()[i].set_title(\"Cat\" if label == 1.0 else \"Dog\")\n",
    "        ax.ravel()[i].imshow(image)\n",
    "        ax.ravel()[i].set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e695e0-b501-4f3f-87fa-5d2930db6570",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "visualize_augmentations(train_dataset, \"Regular dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f8317e-cf1d-4251-a311-2992de0c7b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "visualize_augmentations(train_wrong_labels_dataset, \"Wrong labels dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340ac184-bf1c-4a59-bb20-c63357808642",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "visualize_augmentations(train_noise_dataset, \"Noise dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89522f7-bc50-47f6-8251-aa1fd982ec54",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "visualize_augmentations(train_augmented_dataset, \"Augmented dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d219a7cc-ec20-4022-8d9b-6da4e4f89f6b",
   "metadata": {},
   "source": [
    "# Machine learning theory\n",
    "## Bartosz Adamczyk 148163, Illia Vysochyn 150..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09fbaf1-4d66-4dde-b00d-301711b4873f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = TensorBoardLogger(\"tb_logs\", name=\"mlt_log\")\n",
    "dataloader_train = DataLoader(train_augmented_dataset, batch_size=16, num_workers=24, shuffle=False)\n",
    "dataloader_val = DataLoader(val_dataset, batch_size=16, num_workers=24,shuffle=False)\n",
    "\n",
    "modelT = MobileNetTrainer()\n",
    "# trainer = L.Trainer(max_epochs=26,callbacks=[EarlyStopping(monitor=\"val_loss\",patience=3, mode=\"min\"),LogEpochValidationImage(tiny_imagenet_val)],logger=logger)\n",
    "trainer = L.Trainer(max_epochs=26,callbacks=[EarlyStopping(monitor=\"val_loss\",patience=3, mode=\"min\")],logger=logger)\n",
    "trainer.fit(model=modelT, train_dataloaders=dataloader_train,val_dataloaders=dataloader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6adcc69-ea49-45c9-bce2-aed12cdfccb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-theory",
   "language": "python",
   "name": "machine-learning-theory"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
